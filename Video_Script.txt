[1. Introduction 0:00 – 0:30]

[Dan] Hello!  My name’s Dan, 
[David] and my name is David,
[Brian] and my name is Brian, 

[Max] 
and my name is Max, 

Welcome to our video presentation for the Python Information Aggregator project. 

In this assignment, we developed a Python application that collects, combines, 
and displays news data using both a public Web API and HTML web scraping techniques. 

I'll let David walk you through what we built.


[David]
[2. Objective Overview 0:30 – 1:30]

This project is all about building a flexible Python application that can fetch, combine, 
and display news data using both a public API extraction and html web scraping. What makes it interesting
isn’t just the data but how we built it. This tool was developed using object-oriented programming principles, 
and it features a graphical user interface to make it easier and more engaging to use.

Now, some parts of the code may not be the most elegant or minimal. But that’s by design. The 
assignment asks us to incorporate encapsulation, inheritance, and polymorphism, so instead of coding 
the shortest and most efficient solution, we focused on clear design that demonstrates these concepts in action.

Over to Brian to talk about the key components.

[Brian]
[3. Key Components [1:30 – 3:00]

[API Integration:]

We used the NewsAPI to access structured news articles in JSON format. Users can filter by category, 
country, or news source. This flexibility makes the tool adaptable to a wide range of interests.

[Web Scraping:]

We also implemented scraping of the ABC News Australia website using BeautifulSoup. This enriched our 
dataset with headlines that aren’t available via API and demonstrated 
how to ethically extract real-time public data.

[OOP Principles in Action:]

Our core architecture relies on object-oriented programming. The KeyList class manages API keys, 
and a NewsAPI_KeyList subclass adds functionality like validity checking. This design showcases 
encapsulation, inheritance, and polymorphism in action.

[Unit Testing:]

Testing was a key part of our process. We wrote unit tests using the unittest framework to 
validate individual components — such as key management and data formatting — before moving 
on to more complex integrations like the GUI and network calls.

[GUI Interaction:]

Our user interface was built using Tkinter. It allows users to select API parameters, fetch news, 
and view the results in an intuitive panel. Keyword frequency graphs are automatically generated 
using Matplotlib, giving a visual summary of the dataset.

[Optional Features and Visualizations:]

We added features like word exclusion filters for the graph, independent clearing of API vs scraped articles, 
and a summary panel that tracks source counts and publication date ranges.

Now Dan will finish up and tell you about the Submission Requirements.

[Dan]
[4. Submission Requirements 3:00 – 4:30]

To meet the assignment criteria, we included:

A fully object-oriented Python codebase

Unit tests for key components using unittest

A README file listing required libraries and setup instructions

A Tkinter-based GUI with screenshots

And a short design report covering architecture, design challenges, and extra features like scraping and visualization.

This video presentation is part of those submission deliverables.

[5. Important Notes and Conclusion 4:30 – 5:00]

One final note — ethical scraping is critical. We made sure to respect the terms of service of the ABC News site, 
avoided excessive requests, and didn’t collect sensitive or personal data. As you should always ensure scraping 
practices are legal, minimal, and respectful of content owners.

[6. Closing and Call to Action 5:00 – 5:30]

Thank you for watching our walkthrough of the Python Information Aggregator!

We hope it gave you a clear picture of how we approached this assignment.

Until next time!
